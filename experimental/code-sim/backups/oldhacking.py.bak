import testtokenizer
from collections import Counter

tokenize = testtokenizer.tokenizer('../javahw1/fixed/')
tokenize.folderRead()

tokensumdict = tokenize.summaryDict.keys()

tokensumdict = list(tokensumdict)
vec = j.summaryDict[c[4]][0]
rvec = j.summaryDict[c[4]][1]
vec1 = j.summaryDict[c[3]][0]
rvec1 = j.summaryDict[c[3]][1]
vec = vec.split()
vec1 = vec1.split()
rvec = rvec.split()
rvec1 = rvec1.split()
k = Counter()
z = Counter()


for x, y in enumerate(vec):
    if k.get(y):
        k[y][0].append(x)
        k[y][1] += 1
        continue
    k[y] = [[x], 1]

for x, y in enumerate(vec1):
    if z.get(y):
        z[y][0].append(x)
        z[y][1] += 1
        continue
    z[y] = [[x], 1]
    
matchd = dict()
for key, v in k.items():
    if z.get(key):
        zset = set(z.get(key)[0])
        kset = set(v[0])
        i = zset.intersection(kset)
    if i is not None:
        for ke in i:
            matchd[ke] = key
